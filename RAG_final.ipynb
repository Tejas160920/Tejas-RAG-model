{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot for Portfolio\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot using FAISS for vector search and Groq LLM for responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pdfplumber faiss-cpu numpy sentence-transformers langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "groq_api_key = 'YOUR_GROQ_API_KEY_HERE'  # Replace with your actual API key from https://console.groq.com\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0.3,\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name='llama-3.3-70b-versatile'\n",
    ")\n",
    "\n",
    "pdf_file_path = 'DATA.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and Process PDF\n",
    "def load_pdf_text(pdf_path):\n",
    "    pdf_text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                pdf_text += page_text + '\\n'\n",
    "    return pdf_text\n",
    "\n",
    "def structure_text(text):\n",
    "    sections = text.split('\\n# ')\n",
    "    documents = {}\n",
    "    for section in sections:\n",
    "        lines = section.split('\\n')\n",
    "        title = lines[0].strip()\n",
    "        content = '\\n'.join(lines[1:]).strip()\n",
    "        if title and content:\n",
    "            documents[title] = content\n",
    "    return documents\n",
    "\n",
    "pdf_text = load_pdf_text(pdf_file_path)\n",
    "structured_data = structure_text(pdf_text)\n",
    "\n",
    "print('Structured sections extracted:', len(structured_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "document_keys = list(structured_data.keys())\n",
    "document_texts = [structured_data[key] for key in document_keys]\n",
    "doc_embeddings = embedder.encode(document_texts, convert_to_numpy=True)\n",
    "\n",
    "embedding_dim = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(doc_embeddings)\n",
    "print('FAISS index built with', index.ntotal, 'documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG Prompt\n",
    "MAX_CONTEXT_LENGTH = 5000\n",
    "\n",
    "def build_prompt(query, top_k=5):\n",
    "    query_embedding = embedder.encode(query, convert_to_numpy=True)\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "\n",
    "    retrieved_sections = [document_keys[i] + ':\\n' + document_texts[i] for i in indices[0]]\n",
    "\n",
    "    context = '\\n\\n'.join(retrieved_sections)\n",
    "    if len(context) > MAX_CONTEXT_LENGTH:\n",
    "        context = context[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f'''You are Tejas's personal AI assistant embedded on his portfolio website.\n",
    "\n",
    "FORMATTING RULES:\n",
    "- Use bullet points to list items - avoid long paragraphs\n",
    "- Use relevant emojis to make responses engaging\n",
    "- Keep each bullet point short and scannable\n",
    "- Start with a brief 1-line intro, then use bullets for details\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:'''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG Chatbot\n",
    "query = 'Tell me about Tejas projects'\n",
    "\n",
    "prompt = build_prompt(query, top_k=5)\n",
    "print('=== Prompt Sent to Model ===')\n",
    "print(prompt[:500], '...')\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print('\\n=== Generated Answer ===')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Chat Loop\n",
    "def chat():\n",
    "    print(\"Tejas's Portfolio Chatbot (type 'quit' to exit)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        query = input('\\nYou: ')\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print('Goodbye!')\n",
    "            break\n",
    "        \n",
    "        prompt = build_prompt(query)\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f'\\nAssistant: {response.content}')\n",
    "\n",
    "# Uncomment to run interactive chat\n",
    "# chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
